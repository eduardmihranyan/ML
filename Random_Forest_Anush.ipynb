{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import builtins\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(s):\n",
    "    # TODO: Implement\n",
    "    # You will find this function useful.\n",
    "    return np.exp(s)/(1+np.exp(s))\n",
    "\n",
    "\n",
    "def normalized_gradient(X, Y, beta, l):\n",
    "    \"\"\"\n",
    "    :param X: data matrix (2 dimensional np.array)\n",
    "    :param Y: response variables (1 dimensional np.array)\n",
    "    :param beta: value of beta (1 dimensional np.array)\n",
    "    :param l: regularization parameter lambda\n",
    "    :return: normalized gradient, i.e. gradient normalized according to data\n",
    "    \"\"\"\n",
    "    # TODO: Implement\n",
    "    beta=beta.reshape(beta.shape[0],1)\n",
    "    grad=np.ones(X.shape[1])\n",
    "    for i in range(X.shape[1]):\n",
    "        sum=0\n",
    "        for j in range(X.shape[0]):\n",
    "            sum+=X[j,i]*Y[j]*(1-sigmoid(Y[j]*beta.T.dot(X[j,:])))\n",
    "\n",
    "        grad[i]=2*l*beta[i]-sum\n",
    "\n",
    "    grad/=X.shape[0]\n",
    "\n",
    "    return grad\n",
    "\n",
    "\n",
    "def gradient_descent(X, Y, epsilon=1e-6, l=1, step_size=1e-4, max_steps=1000):\n",
    "    \"\"\"\n",
    "    Implement gradient descent using full value of the gradient.\n",
    "    :param X: data matrix (2 dimensional np.array)\n",
    "    :param Y: response variables (1 dimensional np.array)\n",
    "    :param l: regularization parameter lambda\n",
    "    :param epsilon: approximation strength\n",
    "    :param max_steps: maximum number of iterations before algorithm will\n",
    "        terminate.\n",
    "    :return: value of beta (1 dimensional np.array)\n",
    "    \"\"\"\n",
    "    X_norm=np.copy(X)\n",
    "    X_mean = np.mean(X,axis=0)\n",
    "    X_std=np.std(X,axis=0)\n",
    "    #print(X_std)\n",
    "    for i in range(1,X.shape[1]):\n",
    "        X_norm[:,i]=(X_norm[:,i]-X_mean[i])/X_std[i]\n",
    "    X_var=np.var(X,axis=0)\n",
    "\n",
    "    beta = np.ones(X.shape[1])\n",
    "    for s in range(max_steps):\n",
    "        #if s % 1000 == 0:\n",
    "            #print(s, beta)\n",
    "        # TODO: Implement iterations.\n",
    "        grad_beta = normalized_gradient(X_norm, Y, beta, l)\n",
    "        if np.linalg.norm(step_size * grad_beta) < epsilon:\n",
    "\n",
    "            break\n",
    "        else:\n",
    "            beta = beta - step_size * grad_beta\n",
    "        pass\n",
    "    beta[0] = beta[0] - sum(X_mean[j] * beta[j] / X_std[j] for j in range(1, X.shape[1]))\n",
    "    for i in range(1, X.shape[1]):\n",
    "        beta[i] = beta[i] / X_std[i]\n",
    "    return beta\n",
    "\n",
    "\n",
    "def lr_predict(X, beta):\n",
    "    \"\"\"\n",
    "    :param X: 2 dimensional python list or numpy 2 dimensional array representing the data to be predicted\n",
    "    :param beta: 1 dimentional python list or numpy array representing the weights returned by our model \n",
    "        (in this case logistic regression)\n",
    "    \n",
    "    :return: 1 dimentional python list or numpy array, the predicted labels y_i for correspinding X_i\n",
    "    \"\"\"\n",
    "    teta = [sigmoid(beta.T.dot(x)) for x in X]\n",
    "    y = [1 if t > 0.5 else 0 for t in teta]\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DecisionNode(object):\n",
    "    \"\"\"\n",
    "    README\n",
    "    DecisionNode is a building block for Decision Trees.\n",
    "    DecisionNode is a python class representing a  node in our decision tree\n",
    "    node = DecisionNode()  is a simple usecase for the class\n",
    "    you can also initialize the class like this:\n",
    "    node = DecisionNode(column = 3, value = \"Car\")\n",
    "    In python, when you initialize a class like this, its __init__ method is called \n",
    "    with the given arguments. __init__() creates a new object of the class type, and initializes its \n",
    "    instance attributes/variables.\n",
    "    In python the first argument of any method in a class is 'self'\n",
    "    Self points to the object which it is called from and corresponds to 'this' from Java\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 column=None,\n",
    "                 value=None,\n",
    "                 false_branch=None,\n",
    "                 true_branch=None,\n",
    "                 current_results=None,\n",
    "                 is_leaf=False,\n",
    "                 result=None):\n",
    "        \"\"\"\n",
    "        column is the column of the feature \n",
    "        value is the value of the feature of the specified column\n",
    "        false_branch and true_branch are of type DecisionNode\n",
    "        current_results is a dictionary that shows, for the current node,\n",
    "            how many of each results it has (for example {\"a\":0, \"b\":5, \"c\":45})\n",
    "        is_leaf is boolean and shows if node is a leaf\n",
    "        result is the most popular answer from curren_results. (in the above example \"c\")\n",
    "        \"\"\"\n",
    "        self.column = column\n",
    "        self.value = value\n",
    "        self.false_branch = false_branch\n",
    "        self.true_branch = true_branch\n",
    "        self.current_results = current_results\n",
    "        self.is_leaf = is_leaf\n",
    "        max = -1\n",
    "        for i in current_results.keys():\n",
    "            a = current_results[i]\n",
    "            if a > max:\n",
    "                max = a\n",
    "                self.result = i\n",
    "\n",
    "def dict_of_values(data):\n",
    "    \"\"\"\n",
    "    param data: a 2D Python list representing the data. Last column of data is Y.\n",
    "    return: returns a python dictionary showing how many times each value appears in Y\n",
    "\n",
    "    for example \n",
    "    data = [[1,'yes'],[1,'no'],[1,'yes'],[1,'yes']]\n",
    "    dict_of_values(data)\n",
    "    should return {'yes' : 3, 'no' :1}\n",
    "    \"\"\"\n",
    "    results = defaultdict(int)\n",
    "    for row in data:\n",
    "        r = row[len(row) - 1]\n",
    "        results[r] += 1\n",
    "    return dict(results)\n",
    "\n",
    "\n",
    "def divide_data(data, feature_column, feature_val):\n",
    "    \"\"\"\n",
    "    this function dakes the data and divides it in two parts by a line. A line\n",
    "    is defined by the feature we are considering (feature_column) and the target \n",
    "    value. The function returns a tuple (data1, data2) which are the desired parts of the data.\n",
    "    For int or float types of the value, data1 have all the data with values >= feature_val\n",
    "    in the corresponding column and data2 should have rest.\n",
    "    For string types, data1 should have all data with values == feature val and data2 should \n",
    "    have the rest.\n",
    "\n",
    "    param data: a 2D Python list representing the data. Last column of data is Y.\n",
    "    param feature_column: an integer index of the feature/column.\n",
    "    param feature_val: can be int, float, or string\n",
    "    return: a tuple of two 2D python lists\n",
    "    \"\"\"\n",
    "    data1=[]\n",
    "    data2=[]\n",
    "    if type(feature_val) == int or type(feature_val) == float:\n",
    "        for i in range(len(data)):\n",
    "            if data[i][feature_column]>=feature_val:\n",
    "                data1.append(data[i])\n",
    "            else:\n",
    "                data2.append(data[i])\n",
    "    if type(feature_val) == str:\n",
    "        for i in range(len(data)):\n",
    "            if data[i][feature_column]==feature_val:\n",
    "                data1.append(data[i])\n",
    "            else:\n",
    "                data2.append(data[i])\n",
    "    return (data1, data2)\n",
    "\n",
    "\n",
    "def gini_impurity(data1, data2):\n",
    "\n",
    "    \"\"\"\n",
    "    Given two 2D lists of compute their gini_impurity index. \n",
    "    Remember that last column of the data lists is the Y\n",
    "    Lets assume y1 is y of data1 and y2 is y of data2.\n",
    "    gini_impurity shows how diverse the values in y1 and y2 are.\n",
    "    gini impurity is given by \n",
    "\n",
    "    N1*sum(p_k1 * (1-p_k1)) + N2*sum(p_k2 * (1-p_k2))\n",
    "\n",
    "    where N1 is number of points in data1\n",
    "    p_k1 is fraction of points that have y value of k in data1\n",
    "    same for N2 and p_k2\n",
    "\n",
    "\n",
    "    param data1: A 2D python list\n",
    "    param data2: A 2D python list\n",
    "    return: a number - gini_impurity \n",
    "    \"\"\"\n",
    "    dict1,dict2=dict_of_values(data1),dict_of_values(data2)\n",
    "    k1,k2=list(dict1.values()),list(dict2.values())\n",
    "    N1,N2 = sum(k1),sum(k2)\n",
    "\n",
    "    p_k1,p_k2=[x/N1 for x in k1],[x/N2 for x in k2]\n",
    "\n",
    "    gini1=0\n",
    "    gini2=0\n",
    "    for i in range(len(k1)):\n",
    "        gini1+= N1*(p_k1[i]*(1-p_k1[i]))\n",
    "    for i in range(len(k2)):\n",
    "        gini2+= N2*(p_k2[i]*(1-p_k2[i]))\n",
    "    gini=gini1+gini2\n",
    "\n",
    "    return gini\n",
    "\n",
    "\n",
    "def build_tree(data, current_depth=0, max_depth=1e10):\n",
    "    \"\"\"\n",
    "    build_tree is a recursive function.\n",
    "    What it does in the general case is:\n",
    "    1: find the best feature and value of the feature to divide the data into\n",
    "    two parts\n",
    "    2: divide data into two parts with best feature, say data1 and data2\n",
    "        recursively call build_tree on data1 and data2. this should give as two \n",
    "        trees say t1 and t2. Then the resulting tree should be \n",
    "        DecisionNode(...... true_branch=t1, false_branch=t2) \n",
    "\n",
    "\n",
    "    In case all the points in the data have same Y we should not split any more, and return that node\n",
    "    For this function we will give you some of the code so its not too hard for you ;)\n",
    "    \n",
    "    param data: param data: A 2D python list\n",
    "    param current_depth: an integer. This is used if we want to limit the numbr of layers in the\n",
    "        tree\n",
    "    param max_depth: an integer - the maximal depth of the representing\n",
    "    return: an object of class DecisionNode\n",
    "\n",
    "    \"\"\"\n",
    "    if len(data) == 0:\n",
    "        return DecisionNode(is_leaf=True)\n",
    "\n",
    "    if(current_depth == max_depth):\n",
    "        return DecisionNode(current_results=dict_of_values(data))\n",
    "\n",
    "    if(len(dict_of_values(data)) == 1):\n",
    "        return DecisionNode(current_results=dict_of_values(data), is_leaf=True)\n",
    "\n",
    "    #This calculates gini number for the data before dividing \n",
    "    self_gini = gini_impurity(data, [])\n",
    "\n",
    "    #Below are the attributes of the best division that you need to find. \n",
    "    #You need to update these when you find a division which is better\n",
    "    best_gini = 1e10\n",
    "    best_column = None\n",
    "    best_value = None\n",
    "    best_split = None\n",
    "\n",
    "   \n",
    "    def column(matrix, i):\n",
    "        return [row[i] for row in matrix]\n",
    "\n",
    "    for i in range(len(data[0])-1):\n",
    "        for j in column(data,i):\n",
    "            gini=gini_impurity(divide_data(data,i,j)[0],divide_data(data,i,j)[1])\n",
    "            if gini<best_gini:\n",
    "                best_gini=gini\n",
    "                best_column=i\n",
    "                best_value=j\n",
    "                best_split=(divide_data(data,i,j)[0],divide_data(data,i,j)[1])\n",
    "    #print(best_gini,best_column,best_value,best_split)\n",
    "   \n",
    "    #recursively call build tree, construct the correct return argument and return\n",
    "    t1=build_tree(best_split[1])\n",
    "    t2=build_tree(best_split[0])\n",
    "    return DecisionNode(best_column,best_value,t1,t2, dict_of_values(data))\n",
    "        \n",
    "\n",
    "\n",
    "def print_tree(tree, indent=''):\n",
    "    # Is this a leaf node?\n",
    "    if tree.is_leaf:\n",
    "        print(str(tree.current_results))\n",
    "    else:\n",
    "        # Print the criteria\n",
    "        #         print (indent+'Current Results: ' + str(tree.current_results))\n",
    "        print('Column ' + str(tree.column) + ' : ' + str(tree.value) + '? ')\n",
    "\n",
    "        # Print the branches\n",
    "        print(indent + 'True->', end=\"\")\n",
    "        print_tree(tree.true_branch, indent + '  ')\n",
    "        print(indent + 'False->', end=\"\")\n",
    "        print_tree(tree.false_branch, indent + '  ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DecisionTree(object):\n",
    "    \"\"\"\n",
    "    DecisionTree class, that represents one Decision Tree\n",
    "\n",
    "    :param max_tree_depth: maximum depth for this tree.\n",
    "    \"\"\"\n",
    "    def __init__(self, max_tree_depth):\n",
    "        self.max_depth = max_tree_depth\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        :param X: 2 dimensional python list or numpy 2 dimensional array\n",
    "        :param Y: 1 dimensional python list or numpy 1 dimensional array\n",
    "        \n",
    "        :return: a tree of self.max_tree_depth with helps to classify the data\n",
    "        \"\"\"\n",
    "        \n",
    "            \n",
    "        if type(X) is np.ndarray:\n",
    "            X=X.tolist()\n",
    "        if type(Y) is np.ndarray:\n",
    "            Y=Y.tolist()\n",
    "\n",
    "        data=[X[i]+[Y[i]] for i in range(len(X))]\n",
    "        \n",
    "        self.root=build_tree(data,max_depth=self.max_depth)\n",
    "\n",
    "\n",
    "\n",
    "    def predict_x(self,x, v):\n",
    "        \n",
    "        \"\"\"\n",
    "        :param x: 1 dimensional python list or numpy 2 dimensional array\n",
    "        :param v: the corrent node of the tree\n",
    "       \n",
    "        :return: choose and recursively call on the next node based on the values of x\n",
    "        \"\"\"\n",
    "        \n",
    "        if v.is_leaf==True:\n",
    "            return v.result\n",
    "        else:\n",
    "            xval=x[v.column]\n",
    "            val=v.value\n",
    "            if type(val) == int or type(val) == float:\n",
    "                if xval>val:\n",
    "                    return self.predict_x(x,v.true_branch)\n",
    "                else:\n",
    "                    return self.predict_x(x, v.false_branch)\n",
    "            if type(val)==str:\n",
    "                if xval==val:\n",
    "                    return self.predict_x(x,v.true_branch)\n",
    "                else:\n",
    "                    return self.predict_x(x,v.false_branch)\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        :param X: 2 dimensional python list or numpy 2 dimensional array\n",
    "        calls predict1 and returns the final predicted labels for corresponding X's\n",
    "        \n",
    "        :return: Y - 1 dimension python list with labels\n",
    "        \"\"\"\n",
    "\n",
    "        return [self.predict1(row,self.root) for row in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class RandomForest(object):\n",
    "    \"\"\"\n",
    "    RandomForest a class, that represents Random Forests.\n",
    "\n",
    "    :param num_trees: Number of trees in the random forest\n",
    "    :param max_tree_depth: maximum depth for each of the trees in the forest.\n",
    "    :param ratio_per_tree: ratio of points to use to train each of\n",
    "        the trees.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_trees, max_tree_depth, ratio_per_tree=0.5):\n",
    "        self.num_trees = num_trees\n",
    "        self.max_tree_depth = max_tree_depth\n",
    "        self.ratio_per_tree=ratio_per_tree\n",
    "        self.trees = None\n",
    "        \n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        :param X: 2 dimensional python list or numpy 2 dimensional array\n",
    "        :param Y: 1 dimensional python list or numpy 1 dimensional array\n",
    "        \n",
    "        :return: forest - collection of decision trees\n",
    "        \"\"\"\n",
    "        self.trees = []\n",
    "        np.random.seed(13)\n",
    "        n = len(X)\n",
    "        sz = int(n*self.ratio_per_tree)\n",
    "\n",
    "        if type(X) is np.ndarray:\n",
    "            X=X.tolist()\n",
    "        if type(Y) is np.ndarray:\n",
    "            Y=Y.tolist()\n",
    "            \n",
    "        for i in range(self.num_trees):\n",
    "            idx = np.arange(n)\n",
    "            np.random.shuffle(idx)\n",
    "            X = [X[idx[i]] for i in range(n)]\n",
    "            Y = [Y[idx[i]] for i in range(n)]\n",
    "            X_train = [X[i] for i in range(sz)]\n",
    "            Y_train = [Y[i] for i in range(sz)]\n",
    "            tree = DecisionTree(self.max_tree_depth)\n",
    "            tree.fit(X_train, Y_train)\n",
    "            self.trees.append(tree)\n",
    "        return self.trees \n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        :param X: 2 dimensional python list or numpy 2 dimensional array\n",
    "        :return: (Y, conf), tuple with Y being 1 dimension python\n",
    "        list with labels, and conf being 1 dimensional list with\n",
    "        confidences for each of the labels.\n",
    "        \"\"\"\n",
    "\n",
    "        def column(matrix, i):\n",
    "            return [row[i] for row in matrix]\n",
    "        def most_common(lst):\n",
    "            return max(set(lst), key= lst.count)\n",
    "        Y=[]\n",
    "        predicted=[]\n",
    "        answer=[]\n",
    "        conf=[]\n",
    "        for i in range(self.num_trees):\n",
    "            Y.append(self.trees[i].predict(X))\n",
    "        for i in range(len(Y[0])):\n",
    "            predicted.append(column(Y,i))\n",
    "            answer.append(most_common(predicted[i]))\n",
    "            conf.append(predicted[i].count(answer[i])/len(predicted[i]))\n",
    "\n",
    "        return (answer, conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy_score(Y_true, Y_predict):\n",
    "    \"\"\"\n",
    "    :param Y_true: 2D python list or numpy array\n",
    "    :param Y_predict: 2D python list or numpy array\n",
    "    \n",
    "    :return: integer that represents the accurasy - the percent of correctly predicted labels\n",
    "    \"\"\"\n",
    "    accuracy=0\n",
    "    for i in range(len(Y_true)):\n",
    "        if Y_predict[i]==Y_true[i]:\n",
    "            accuracy+=1\n",
    "    return accuracy/len(Y_true)\n",
    "\n",
    "\n",
    "def evaluate_performance():\n",
    "    '''\n",
    "    Evaluate the performance of decision trees and logistic regression,\n",
    "    average over 1,000 trials of 10-fold cross validation\n",
    "\n",
    "    Return:\n",
    "      a matrix giving the performance that will contain the following entries:\n",
    "      stats[0,0] = mean accuracy of decision tree\n",
    "      stats[0,1] = std deviation of decision tree accuracy\n",
    "      stats[1,0] = mean accuracy of logistic regression\n",
    "      stats[1,1] = std deviation of logistic regression accuracy\n",
    "\n",
    "    ** Note that your implementation must follow this API**\n",
    "    '''\n",
    "\n",
    "    # Load Data\n",
    "    filename = 'P3/SPECTF.dat'\n",
    "    data = np.loadtxt(filename, delimiter=',')\n",
    "    X = data[:, 1:]\n",
    "    y = np.array(data[:, 0])\n",
    "    n, d = X.shape\n",
    "    folds=10\n",
    "\n",
    "    decision_tree_accuracies=[]\n",
    "    random_forest_accuracies=[]\n",
    "    log_regression_accuracies=[]\n",
    "\n",
    "    for trial in range(3):\n",
    "        # TODO: shuffle for each of the trials.\n",
    "        # the following code is for reference only.\n",
    "        idx = np.arange(n)\n",
    "        np.random.seed(13)\n",
    "        np.random.shuffle(idx)\n",
    "        X = X[idx]\n",
    "        y = y[idx]\n",
    "        size=int(n-n/folds)\n",
    "\n",
    "        print(\"trial\", trial)\n",
    "\n",
    "        # TODO: write your own code to split data (for cross validation)\n",
    "        # the code here is for your reference.\n",
    "        Xtrain = X[:size,:]# train on first 100 instances\n",
    "        Xtest = X[size:,:]\n",
    "        ytrain = y[:size]# test on remaining instances\n",
    "        ytest = y[size:]\n",
    "\n",
    "        # train the decision tree\n",
    "        dt = DecisionTree(100)\n",
    "        \n",
    "        dt.fit(Xtrain, ytrain)\n",
    "\n",
    "        # output predictions on the remaining data\n",
    "        dt_pred = dt.predict(Xtest)\n",
    "        dt_accuracy = accuracy_score(ytest, dt_pred)\n",
    "        decision_tree_accuracies.append(dt_accuracy)\n",
    "\n",
    "        #train random forest\n",
    "        rf= RandomForest(10,100)\n",
    "        rf.fit(Xtrain,ytrain)\n",
    "        rf_pred= rf.predict(Xtest)[0]\n",
    "        rf_accuracy= accuracy_score(ytest,rf_pred)\n",
    "        random_forest_accuracies.append(rf_accuracy)\n",
    "\n",
    "        #logistic regression\n",
    "        lr_beta=gradient_descent(Xtrain,ytrain,step_size=1e-1,max_steps=100)\n",
    "\n",
    "        lr_pred=lr_predict(Xtest,lr_beta)\n",
    "        lr_accuracy=accuracy_score(ytest,lr_pred)\n",
    "        log_regression_accuracies.append(lr_accuracy)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # compute the training accuracy of the model\n",
    "    meanDecisionTreeAccuracy = np.mean(decision_tree_accuracies)\n",
    "    stddevDecisionTreeAccuracy = np.std(decision_tree_accuracies)\n",
    "    meanLogisticRegressionAccuracy = np.mean(log_regression_accuracies)\n",
    "    stddevLogisticRegressionAccuracy = np.std(log_regression_accuracies)\n",
    "    meanRandomForestAccuracy = np.mean(random_forest_accuracies)\n",
    "    stddevRandomForestAccuracy = np.std(random_forest_accuracies)\n",
    "\n",
    "    # make certain that the return value matches the API specification\n",
    "    stats = np.zeros((3, 2))\n",
    "    stats[0, 0] = meanDecisionTreeAccuracy\n",
    "    stats[0, 1] = stddevDecisionTreeAccuracy\n",
    "    stats[1, 0] = meanRandomForestAccuracy\n",
    "    stats[1, 1] = stddevRandomForestAccuracy\n",
    "    stats[2, 0] = meanLogisticRegressionAccuracy\n",
    "    stats[2, 1] = stddevLogisticRegressionAccuracy\n",
    "    return stats\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trial 0\n",
      "trial 1\n",
      "trial 2\n",
      "Decision Tree Accuracy =  0.728395061728  ( 0.0461933010713 )\n",
      "Random Forest Tree Accuracy =  0.79012345679  ( 0.0761038765799 )\n",
      "Logistic Reg. Accuracy =  0.197530864198  ( 0.0872971334798 )\n"
     ]
    }
   ],
   "source": [
    "stats = evaluate_performance()\n",
    "print (\"Decision Tree Accuracy = \", stats[0, 0], \" (\", stats[0, 1], \")\")\n",
    "print (\"Random Forest Tree Accuracy = \", stats[1, 0], \" (\", stats[1, 1], \")\")\n",
    "print (\"Logistic Reg. Accuracy = \", stats[2, 0], \" (\", stats[2, 1], \")\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
